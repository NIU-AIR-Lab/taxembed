---
title: "SkeletonKey"
author: "AlexanderWold_Z1817662"
date: "1/19/2021"
output: pdf_document
---

This file automatically handles all of the validation tests - word pairs and analogies

Then, based off of that information, it handles the variable tests to see which changes among the embeddings actually make a difference

```{r}
library(reticulate)
library(tibble)
library(stringr)
library(broom)
library(purrr)
library(dplyr)
library(pkgcond)

VAR_PATH <- "/home/alex/Projects/taxembed/2_validation/SkeletonKey/output"
VAR_OUTPUT_PATH <- "/home/alex/Projects/taxembed/2_validation/SkeletonKey/variables"
```


MODULES
```{python}
import os
import re
import gensim
import pandas as pd
import sys
```


LIST OF CHANGEABLE CONSTANTS
```{python}
# define important input paths the user should change
MODEL_PATH = "/home/alex/Projects/taxembed/data/embeddings"
ANALOGY_PATH = "/home/alex/Projects/taxembed/2_validation/SkeletonKey/input/analogies"
PAIR_PATH = "/home/alex/Projects/taxembed/2_validation/SkeletonKey/input/wordpairs"
VAR_PATH = "/home/alex/Projects/taxembed/2_validation/SkeletonKey/output"

# define important output paths the user should change
ANALOGY_OUTPUT_PATH = "/home/alex/Projects/taxembed/2_validation/SkeletonKey/output/analogies"
PAIR_OUTPUT_PATH = "/home/alex/Projects/taxembed/2_validation/SkeletonKey/output/pairs"

# define important flags the user should change
NGRAMS = False
```


MODELS
```{python}
# get the list of models from the user specified path
# use list comprehension tp get a list of models within the 'models' folder
# do not list the peripheral .npy files
model_list = [file for file in os.listdir(MODEL_PATH) if re.match("^(?:(?!\.).)*$", file)]

if NGRAMS == True:
    model_list = [modelname for modelname in model_list if re.search("(?<!no_)ngrams", modelname)]

# create an empty list to hold the path names for each model
model_paths = []

# append the current working directory to each model name
for model_name in model_list:
    model_paths.append(os.path.join(MODEL_PATH, model_name))
```


ANALOGIES
```{python}
# create an object to hold the analogy file names
analogy_list = []

# recursively look in the analogies path for .txt files
for root, dirs, files in os.walk(ANALOGY_PATH):
  for name in files:
    analogy_list.append(os.path.join(root, name))
    
# get rid of any ngram sets if NGRAMS is set to false:
# NOTE: analogy files that have 'ngrams' in the file name will NOT be included
if NGRAMS == False:
  analogy_list = [analogy_name for analogy_name in analogy_list if not re.search("ngram", analogy_name)]
```

FUNCTIONS
```{python}
################################################################
#           |                                                  #
# FUNCTION: | store_analogy_eval                               #
#-----------|--------------------------------------------------#
# USE:      | stores the results of an evaluation object into  #
#           | a dictionary (for analogies)                     #
#           |                                                  #
################################################################
def store_analogy_eval(analogy_eval_object):

    # create a more discriptive dictionary for the values of a word pair evaluation
    eval_dict = {"Overall Analogy Accuracy (%)" : round((analogy_eval_object[0] * 100), 2)}
    
    return eval_dict

################################################################
#           |                                                  #
# FUNCTION: | perform_analogy_evals                            #
#-----------|--------------------------------------------------#
# USE:      | systematically loads embedding models and grabs  #
#           | the word evaluation information. This function   #
#           | returns a list of pandas dataframes              #
#           |                                                  #
################################################################
def perform_analogy_evals(model_path_list, analogy_path_list):
  
  # create an empty storage list to hold the evaluation dataframes
  evaluation_list = []
  
  # loop through the analogy paths
  for analogy_path in analogy_path_list:
    
    # create an empty list to store the evaluation dictionaries
    analogy_eval_list = []
    
    # create an empty list to store dictionaries built from model file names
    name_info_dicts = []
    
    for path in model_path_list:
      # get the name information
      name_info_dicts.append(name_parse(path))
      
      # evaluate the analogies
      loaded_model = gensim.models.Word2Vec.load(path)
      analogy_eval_list.append(store_analogy_eval(loaded_model.wv.evaluate_word_analogies(analogy_path)))
      del loaded_model
    
    # merge the summary evaluation statistics with the name information
    for i in range(len(name_info_dicts)):
        name_info_dicts[i].update(analogy_eval_list[i])
     
    # create a list of key names (column names) and an empty list to hold header names
    column_names = name_info_dicts[0].keys()
    header_names = []
    
    # take the key names and replace all spaces with \n, this will allow for better printing with tabulate
    for col_name in column_names:
        header_names.append(re.sub(pattern = "\s", repl = "\n", string = col_name))
    
    # create an empty data frame with the specified key (column) names
    info_df = pd.DataFrame(columns = column_names)
    
    # append the information to the data frame
    for i in range(len(name_info_dicts)):
        info_df = info_df.append(name_info_dicts[i], ignore_index = True)
    
    evaluation_list.append(info_df)
        
  return evaluation_list
    
################################################################
#           |                                                  #
# FUNCTION: | name_parse                                      #
#-----------|--------------------------------------------------#
# USE:      | grabs information from the model file name and   #
#           | returns a dictionary with all of the information #
#           |                                                  #
################################################################
def name_parse(model_path):
  
  # get the model name from the model path
  model_name = (model_path.split("/"))[-1]
  
  stopwords_flag = False
  code_refs_flag = False
  embedding_type_flag = "Word2Vec"
  ngrams_flag = False
  
  # check if the model used stopwords
  if re.search(pattern = "nosw_", string = model_name) == None:
      stopwords_flag = True
  
  # check if the model preserved custom references
  if re.search(pattern = "_refs_", string = model_name) != None:
      code_refs_flag = True
      
  # check the algorithm used for training
  if re.search(pattern = "_ft_", string = model_name) != None:
      embedding_type_flag = "FastText"
      
  else:
      embedding_type_flag = "Word2Vec"
      
  # check if the model preserved ngrams
  if re.search(pattern = "_no_ngrams_", string = model_name) == None:
      ngrams_flag = True
  
  
  # grab information for the dimensions
  dimensions_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "d_\d{1,3}_",
                                                                          string = model_name).group(0)).group(0))
  
  # grab information for the window
  window_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "_w_\d{1,3}", string = model_name).group(0)).group(0))
  
  # grab information for the min word occur
  mwo_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "_mwo_\d{1,3}", string = model_name).group(0)).group(0))
  
  # grab information for the epochs
  epoch_flag = int(re.search(pattern = "\d{1,4}", string = re.search(pattern = "_e_\d{1,4}", string = model_name).group(0)).group(0))
  
  
  # create a dictionary with all of the information
  title_dict = {"Model Name" : model_name, "Stopwords?" : stopwords_flag, "References Preserved?" : code_refs_flag,
               "Training Algorithm" : embedding_type_flag, "Ngrams?" : ngrams_flag,
               "Dimensions" : dimensions_flag, "Window" : window_flag,
               "Minimum Count Threshold" : mwo_flag, "Number of Training Epochs" : epoch_flag}
  
  return(title_dict)  
```

EVALUATION
```{python}
# run the validation functions to get analogy accuracy scores
analogy_results = perform_analogy_evals(model_paths, analogy_list)
```

TO CSV
```{python}
for i in range(len(analogy_list)):
  # get the analogy file name from the analogy path
  analogy_name = (analogy_list[i].split("/"))[-1]
  
  # strip the extension from the analogy path
  analogy_name = (analogy_name.split("."))[0]
  
  # mark the output file as "AER" for analogy evaluation results
  analogy_name = analogy_name + "_AER.csv"
  
  # complete the output path
  output_path = os.path.join(ANALOGY_OUTPUT_PATH, analogy_name)
  
  # save the evaluation data to a CSV
  analogy_results[i].to_csv(output_path)
```








PAIRS
```{python}
# create an object to hold the analogy file names
pair_list = []

# recursively look in the analogies path for .txt files
for root, dirs, files in os.walk(PAIR_PATH):
  for name in files:
    pair_list.append(os.path.join(root, name))
    
# get rid of any ngram sets if NGRAMS is set to false:
# NOTE: analogy files that have 'ngrams' in the file name will NOT be included
if NGRAMS == False:
  pair_list = [pair_name for pair_name in pair_list if not re.search("ngram", pair_name)]
```

```{python}
################################################################
#           |                                                  #
# FUNCTION: | store_pair_eval                                  #
#-----------|--------------------------------------------------#
# USE:      | stores the results of an evaluation object into  #
#           | a dictionary (for analogies)                     #
#           |                                                  #
################################################################
def store_pair_eval(pair_eval_object):
  eval_dict = {"Pearson Correlation Coefficient" : round(pair_eval_object[0][0], 4),
               "Pearson Correlation Coefficient P-Value" : round(pair_eval_object[0][1], 4),
               "Spearman Correlation Coefficient" : round(pair_eval_object[1].correlation, 4),
               "Spearman Correlation Coefficient P-Value" : round(pair_eval_object[1].pvalue, 4),
               "Unknown Word Pairs %" : round(pair_eval_object[2], 2)}
  
  return eval_dict

################################################################
#           |                                                  #
# FUNCTION: | perform_pair_evals                               #
#-----------|--------------------------------------------------#
# USE:      | systematically loads embedding models and grabs  #
#           | the word evaluation information. This function   #
#           | returns a list of pandas dataframes              #
#           |                                                  #
################################################################
def perform_pair_evals(model_path_list, pair_path_list):
  
  # create an empty storage list to hold the evaluation dataframes
  evaluation_list = []
  
  # loop through the analogy paths
  for pair_path in pair_path_list:
    
    # create an empty list to store the evaluation dictionaries
    pair_eval_list = []
    
    # create an empty list to store dictionaries built from model file names
    name_info_dicts = []
    
    for path in model_path_list:
      # get the name information
      name_info_dicts.append(name_parse(path))
      
      # evaluate the analogies
      loaded_model = gensim.models.Word2Vec.load(path)
      pair_eval_list.append(store_pair_eval(loaded_model.wv.evaluate_word_pairs(pair_path)))
      del loaded_model
    
    # merge the summary evaluation statistics with the name information
    for i in range(len(name_info_dicts)):
        name_info_dicts[i].update(pair_eval_list[i])
     
    # create a list of key names (column names) and an empty list to hold header names
    column_names = name_info_dicts[0].keys()
    header_names = []
    
    # take the key names and replace all spaces with \n, this will allow for better printing with tabulate
    for col_name in column_names:
        header_names.append(re.sub(pattern = "\s", repl = "\n", string = col_name))
    
    # create an empty data frame with the specified key (column) names
    info_df = pd.DataFrame(columns = column_names)
    
    # append the information to the data frame
    for i in range(len(name_info_dicts)):
        info_df = info_df.append(name_info_dicts[i], ignore_index = True)
    
    evaluation_list.append(info_df)
        
  return evaluation_list
    
################################################################
#           |                                                  #
# FUNCTION: | name_parse                                       #
#-----------|--------------------------------------------------#
# USE:      | grabs information from the model file name and   #
#           | returns a dictionary with all of the information #
#           |                                                  #
################################################################
def name_parse(model_path):
  
  # get the model name from the model path
  model_name = (model_path.split("/"))[-1]
  
  stopwords_flag = False
  code_refs_flag = False
  embedding_type_flag = "Word2Vec"
  ngrams_flag = False
  
  # check if the model used stopwords
  if re.search(pattern = "nosw_", string = model_name) == None:
      stopwords_flag = True
  
  # check if the model preserved custom references
  if re.search(pattern = "_refs_", string = model_name) != None:
      code_refs_flag = True
      
  # check the algorithm used for training
  if re.search(pattern = "_ft_", string = model_name) != None:
      embedding_type_flag = "FastText"
      
  else:
      embedding_type_flag = "Word2Vec"
      
  # check if the model preserved ngrams
  if re.search(pattern = "_no_ngrams_", string = model_name) == None:
      ngrams_flag = True
  
  
  # grab information for the dimensions
  dimensions_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "d_\d{1,3}_",
                                                                          string = model_name).group(0)).group(0))
  
  # grab information for the window
  window_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "_w_\d{1,3}", string = model_name).group(0)).group(0))
  
  # grab information for the min word occur
  mwo_flag = int(re.search(pattern = "\d{1,3}", string = re.search(pattern = "_mwo_\d{1,3}", string = model_name).group(0)).group(0))
  
  # grab information for the epochs
  epoch_flag = int(re.search(pattern = "\d{1,4}", string = re.search(pattern = "_e_\d{1,4}", string = model_name).group(0)).group(0))
  
  
  # create a dictionary with all of the information
  title_dict = {"Model Name" : model_name, "Stopwords?" : stopwords_flag, "References Preserved?" : code_refs_flag,
               "Training Algorithm" : embedding_type_flag, "Ngrams?" : ngrams_flag,
               "Dimensions" : dimensions_flag, "Window" : window_flag,
               "Minimum Count Threshold" : mwo_flag, "Number of Training Epochs" : epoch_flag}
  
  return(title_dict)
```


```{python}
pair_results = perform_pair_evals(model_paths, pair_list)
```

```{python}
for i in range(len(pair_list)):
  # get the analogy file name from the analogy path
  pair_name = (pair_list[i].split("/"))[-1]
  
  # strip the extension from the analogy path
  pair_name = (pair_name.split("."))[0]
  
  # mark the output file as "PER" for pair evaluation results
  pair_name = pair_name + "_PER.csv"
  
  # complete the output path
  output_path = os.path.join(PAIR_OUTPUT_PATH, pair_name)
  
  # save the evaluation data to a CSV
  pair_results[i].to_csv(output_path)
```





VARIABLE TESTS
```{r}
################################################################
#           |                                                  #
# FUNCTION: | model_aov                                        #
#-----------|--------------------------------------------------#
# USE:      | this function reads model correlation data from  #
#           | a csv file and performs aov operations on each   #
#           | variable to see if there's a significant         #
#           | difference between variations in the word        #
#           | embedding                                        #
#           |                                                  #
################################################################
model_aov <- function(df)
{
  anova_pearson <- list()
  anova_spearman <- list()
  anova_analogy <- list()
  return_list <- list()

  # anova table for the stop words
  if(nlevels(df$Stopwords.) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Stopwords"]] <- aov(Pearson.Correlation.Coefficient ~ Stopwords., data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Stopwords"]] <- aov(Spearman.Correlation.Coefficient ~ Stopwords., data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Stopwords"]] <- aov(Overall.Analogy.Accuracy.... ~ Stopwords., data = df)
    }
  }

  # anova table for references
  if (nlevels(df$References.Preserved.) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["References"]] <- aov(Pearson.Correlation.Coefficient ~ References.Preserved., data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["References"]] <- aov(Spearman.Correlation.Coefficient ~ References.Preserved., data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["References"]] <- aov(Overall.Analogy.Accuracy.... ~ References.Preserved., data = df)
    }
  }

  # anova table for the training algorithm
  if (nlevels(df$Training.Algorithm) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Algorithm"]] <- aov(Pearson.Correlation.Coefficient ~ Training.Algorithm, data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Algorithm"]] <- aov(Spearman.Correlation.Coefficient ~ Training.Algorithm, data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Algorithm"]] <- aov(Overall.Analogy.Accuracy.... ~ Training.Algorithm, data = df)
    }
  }
  
  # anova table for the ngrams
  if (nlevels(df$Ngrams.) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Ngram"]] <- aov(Pearson.Correlation.Coefficient ~ Ngrams., data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Ngram"]] <- aov(Spearman.Correlation.Coefficient ~ Ngrams., data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Ngram"]] <- aov(Overall.Analogy.Accuracy.... ~ Ngrams., data = df)
    }
  }

  # anova table for the number of dimensions
  if (nlevels(df$Dimensions) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Dimensions"]] <- aov(Pearson.Correlation.Coefficient ~ Dimensions, data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Dimensions"]] <- aov(Spearman.Correlation.Coefficient ~ Dimensions, data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Dimensions"]] <- aov(Overall.Analogy.Accuracy.... ~ Dimensions, data = df)
    }
  }

  # anova table for the window size
  if (nlevels(df$Window) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Window"]] <- aov(Pearson.Correlation.Coefficient ~ Window, data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Window"]] <- aov(Spearman.Correlation.Coefficient ~ Window, data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Window"]] <- aov(Overall.Analogy.Accuracy.... ~ Window, data = df)
    }
  }

  # anova table for the minimum count threshold
  if (nlevels(df$Minimum.Count.Threshold) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Threshold"]] <- aov(Pearson.Correlation.Coefficient ~ Minimum.Count.Threshold, data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Threshold"]] <- aov(Spearman.Correlation.Coefficient ~ Minimum.Count.Threshold, data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Threshold"]] <- aov(Overall.Analogy.Accuracy.... ~ Minimum.Count.Threshold, data = df)
    }
  }

  # anova table for the numnber of training epochs
  if (nlevels(df$Number.of.Training.Epochs) >= 2)
  {
    if ("Pearson.Correlation.Coefficient" %in% colnames(df)) {
      anova_pearson[["Epochs"]] <- aov(Pearson.Correlation.Coefficient ~ Number.of.Training.Epochs, data = df)
    }
    if ("Spearman.Correlation.Coefficient" %in% colnames(df)) {
      anova_spearman[["Epochs"]] <- aov(Spearman.Correlation.Coefficient ~ Number.of.Training.Epochs, data = df)
    }
    if ("Overall.Analogy.Accuracy...." %in% colnames(df)) {
      anova_analogy[["Epochs"]] <- aov(Overall.Analogy.Accuracy.... ~ Number.of.Training.Epochs, data = df)
    }
  }

  # return the appropriate data based on the pearson and spearman argument flags
  if ("Pearson.Correlation.Coefficient" %in% colnames(df))
  {
    return_list[["pearson"]] <- anova_pearson
  }

  if ("Spearman.Correlation.Coefficient" %in% colnames(df))
  {
    return_list[["spearman"]] <- anova_spearman
  }

  if ("Overall.Analogy.Accuracy...." %in% colnames(df))
  {
    return_list[["analogy"]] <- anova_analogy
  }

  return_list
}


################################################################
#           |                                                  #
# FUNCTION: | apply_tukey                                      #
#-----------|--------------------------------------------------#
# USE:      | given a list of aov fitted objects, this         #
#           | function applies the tukeyHSD function to each   #
#           | element                                          #
#           |                                                  #
################################################################
apply_tukey <- function(aov_list)
{
  tukey_list <- lapply(aov_list, TukeyHSD)
  tukey_list
}


################################################################
#           |                                                  #
# FUNCTION: | get_tukey                                        #
#-----------|--------------------------------------------------#
# USE:      | given a list of aov lists, this function applies #
#           | the apply_tukey function to each element         #
#           |                                                  #
################################################################
get_tukey <- function(list_of_aov_lists)
{
  tukey_list <- lapply(list_of_aov_lists, apply_tukey)
  tukey_list
}

################################################################
#           |                                                  #
# FUNCTION: | get_broom                                        #
#-----------|--------------------------------------------------#
# USE:      | given a list of tukey outputs, this function     #
#           | uses the tidy function and the reduce function   #
#           | to display the variable tests in an easy-to-read #
#           | format                                           #
#           |                                                  #
################################################################
get_broom <- function(list_of_tukey_tables)
{
  broom_out <- invisible(lapply(list_of_tukey_tables, tidy))
  broom_out <- suppress_messages(reduce(broom_out, full_join))
  broom_out
}

################################################################
#           |                                                  #
# FUNCTION: | print_tibble                                     #
#-----------|--------------------------------------------------#
# USE:      | given a list of tibbles, this function           #
#           | prints them out, one by one                      #
#           |                                                  #
################################################################
print_tibble <- function(list_of_tibbles)
{
  # loop through the provided list and print the tibble information
  for(i in seq(1, length(list_of_tibbles)))
  {
    print(names(list_of_tibbles)[[i]])
    
    print_frame <- as.data.frame(list_of_tibbles[[i]]) %>% mutate_if(is.numeric, round, digits = 4)
    
    print(print_frame)
    cat("\n\n\n")
  }
}
```


VARIABLE TESTS (TIBBLE OUTPUT WILL ONLY GO TO FILE IN A SCRIPT --- NOT IN RMARKDOWN)
```{r}
# recursively acquire the names of all evaluations, 
all_evals <- list.files(VAR_PATH, full.names = TRUE, recursive = TRUE)

# extract a list of names for the evaluation files
var_paths <- c()
for (i in seq(1, length(all_evals))) {
  ename <- tail(unlist(str_split(string = all_evals[i], pattern = "/")), n = 1)
  ename <- head(unlist(str_split(string = ename, pattern = "\\.")), n = 1)
  ename <- paste0(ename, "_VARTEST.txt")
  ename <- paste0(VAR_OUTPUT_PATH, "/", ename)
  var_paths <- append(var_paths, ename)
}

for (k in seq(1, length(all_evals))) {
  cor_frame <- read.csv(all_evals[k], header = TRUE)
  possible_factors <- c("Dimensions", "Window", "Minimum.Count.Threshold", "Number.of.Training.Epochs")
  col_factors <- possible_factors[possible_factors %in% names(cor_frame)]
  cor_frame[ , col_factors] <- data.frame(apply(cor_frame[col_factors], 2, as.factor))
  
  # get the tukeyHSD information
  variables_aov <- model_aov(cor_frame)
  hsd_list <- get_tukey(variables_aov)
  out_list <- lapply(hsd_list, get_broom)
  
  # print the tukeyHSD information
  options(width = 1000)
  sink(var_paths[k])
  print_tibble(out_list)
  sink()
}
```



```{python}
```

```{python}
```

```{python}
```

